---- Feature extraction & Data Munging --------------

val rdd = sc.textFile("churn/Churn_Modelling.csv").filter( x => ! x.contains("RowNumber"))

val rdd1 = rdd.map( x => x.split(",")).map( x => x.slice(3,x.size))

rdd1.take(5)
res2: Array[Array[String]] = Array(Array(619, France, Female, 42, 2, 0, 1, 1, 1, 101348.88, 1), Array(608, Spain, Female, 41, 1, 83807.86, 1, 0, 1, 112542.58, 0), Array(502, France, Female, 42, 8, 159660.8, 3, 1, 0, 113931.57, 1), Array(699, France, Female, 39, 1, 0, 2, 0, 0, 93826.63, 0), Array(850, Spain, Female, 43, 2, 125510.82, 1, 1, 1, 79084.1, 0))

---- Conversion to 1-to-k binary encoding vectors 

def oneHotEncColumns(rddx: org.apache.spark.rdd.RDD[Array[String]], idx: Int):org.apache.spark.rdd.RDD[Array[Double]] = {
  val categories = rddx.map(r => r(idx)).distinct.zipWithIndex.collect.toMap
  val numCategories = categories.size
  val vetcateg = rddx.map(r => {
      val categoryIdx = categories(r(idx)).toInt
      val categoryFeatures = Array.ofDim[Double](numCategories)
      categoryFeatures(categoryIdx) = 1.0
      categoryFeatures
  })
  vetcateg
}

def mergeArray(rddx: org.apache.spark.rdd.RDD[Array[String]], idx: Int*):org.apache.spark.rdd.RDD[Array[Double]] = {
  var i = 0
  var arr1 = oneHotEncColumns(rddx,idx(i))
  for (j <- 1 until idx.size) {
    var arr2 = oneHotEncColumns(rddx,idx(j))
    var flt1 = arr1.zip(arr2).map(x => (x._1.toList ++ x._2.toList).toArray)
    arr1 = flt1
  }
  arr1
}

val concat = mergeArray(rdd1,1,2)

val rdd2 = rdd1.map( x => Array(x(0).toDouble,x(3).toDouble,x(4).toDouble,x(5).toDouble,x(6).toDouble,x(7).toDouble,x(8).toDouble,x(9).toDouble,x(10).toDouble))

val vect = concat.zip(rdd2).map(x => (x._1.toList ++ x._2.toList).toArray)

import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.mllib.regression.LabeledPoint

val data = vect.map(x => {
  val arr_size = x.size - 1
  val l = x(arr_size)
  val f = x.slice(0,arr_size - 1)
  LabeledPoint(l, Vectors.dense(f))
})

data.cache

val sets = data.randomSplit(Array(0.8,0.2))
val trainSet = sets(0)
val testSet = sets(1)

---- MLlib logistic regression --------------

import org.apache.spark.mllib.classification.LogisticRegressionWithSGD
val numIterations = 100
val model = LogisticRegressionWithSGD.train(trainSet, numIterations)

val validPredicts = testSet.map(x => (model.predict(x.features),x.label))

validPredicts.take(20)
res9: Array[(Double, Double)] = Array((0.0,0.0), (0.0,1.0), (0.0,1.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0))

import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics
val metrics = new BinaryClassificationMetrics(validPredicts)
validPredicts.filter(x => x._1 == x._2).count  // 1596
validPredicts.count                            // 1989
model.getClass.getSimpleName
metrics.areaUnderPR   // 0.1975867269984917
metrics.areaUnderROC  // 0.5

---- MLlib SVM regression --------------

import org.apache.spark.mllib.classification.SVMWithSGD
val numIterations = 100
val model = SVMWithSGD.train(trainSet, numIterations)

val validPredicts = testSet.map(x => (model.predict(x.features),x.label))

import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics
val metrics = new BinaryClassificationMetrics(validPredicts)
validPredicts.filter(x => x._1 == x._2).count  // 1596
validPredicts.count                            // 1989
model.getClass.getSimpleName
metrics.areaUnderPR   // 0.1975867269984917
metrics.areaUnderROC  // 0.5

---- Analyzing statistics for standardization ---------------------

import org.apache.spark.mllib.linalg.Matrix
import org.apache.spark.mllib.linalg.distributed.RowMatrix

val vectors = data.map{ case LabeledPoint(x,y) => y }
val matrix = new RowMatrix(vectors)
val matrixSummary = matrix.computeColumnSummaryStatistics()

matrixSummary.max
res15: org.apache.spark.mllib.linalg.Vector = [1.0,1.0,1.0,1.0,1.0,850.0,92.0,10.0,250898.09,4.0,1.0,1.0]

matrixSummary.min
res16: org.apache.spark.mllib.linalg.Vector = [0.0,0.0,0.0,0.0,0.0,350.0,18.0,0.0,0.0,1.0,0.0,0.0]

matrixSummary.mean
res17: org.apache.spark.mllib.linalg.Vector = [0.5014,0.2477,0.2509,0.5457,0.4543,650.5288000000023,38.92179999999989,5.012799999999994,76485.8892879998,1.530200000000003,0.7055,0.5151]

matrixSummary.variance
res18: org.apache.spark.mllib.linalg.Vector = [0.2500230423042304,0.18636334633463347,0.18796798679867988,0.24793630363036304,0.24793630363036304,9341.860156575658,109.99408416841639,8.364672627262724,3.893436175990728E9,0.33832179217921915,0.20779052905290532,0.2497969696969697]


----- Standardizing features ------------------------------

import org.apache.spark.mllib.feature.StandardScaler
val vectors = trainSet.map(lp => lp.features)
val scaler = new StandardScaler(withMean = true, withStd = true).fit(vectors)
val trainScaled = trainSet.map(lp => LabeledPoint(lp.label,scaler.transform(lp.features)))

----- with MLlib logistic regression ----------------------

import org.apache.spark.mllib.classification.LogisticRegressionWithSGD
val numIterations = 100
val model = LogisticRegressionWithSGD.train(trainScaled, numIterations)

val validPredicts = testSet.map(x => (model.predict(scaler.transform(x.features)),x.label))

import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics
val metrics = new BinaryClassificationMetrics(validPredicts)
validPredicts.filter(x => x._1 == x._2).count  // 1280
validPredicts.count                            // 1989
model.getClass.getSimpleName
metrics.areaUnderPR    // 0.31164061062689297
metrics.areaUnderROC   // 0.687737951749603

----- with MLlib SVM regression ----------------------

import org.apache.spark.mllib.classification.SVMWithSGD
val numIterations = 100
val model = SVMWithSGD.train(trainScaled, numIterations)

val validPredicts = testSet.map(x => (model.predict(scaler.transform(x.features)),x.label))

import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics
val metrics = new BinaryClassificationMetrics(validPredicts)
validPredicts.filter(x => x._1 == x._2).count  // 1312
validPredicts.count                            // 1989
model.getClass.getSimpleName
metrics.areaUnderPR    // 0.320080380254096
metrics.areaUnderROC   // 0.6939270887141519
